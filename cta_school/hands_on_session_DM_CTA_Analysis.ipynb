{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ab970f",
   "metadata": {
    "id": "71ab970f"
   },
   "source": [
    "# Important importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730d667",
   "metadata": {
    "id": "c730d667"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.pyplot import gca\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units import Quantity\n",
    "from astropy.io import fits\n",
    "from regions import CircleSkyRegion\n",
    "\n",
    "from gammapy.irf import load_cta_irfs\n",
    "from gammapy.maps import WcsGeom, MapAxis, WcsNDMap\n",
    "from gammapy.modeling.models import (PowerLawSpectralModel, \n",
    "                                     TemplateSpatialModel, \n",
    "                                     PointSpatialModel, \n",
    "                                     SkyModel, \n",
    "                                     Models, \n",
    "                                     FoVBackgroundModel)\n",
    "\n",
    "from gammapy.makers import MapDatasetMaker\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.data import Observation\n",
    "from gammapy.datasets import MapDataset, FluxPointsDataset\n",
    "from gammapy.estimators import FluxPointsEstimator\n",
    "\n",
    "from gammapy.astro.darkmatter import (profiles, \n",
    "                                      JFactory, \n",
    "                                      PrimaryFlux, \n",
    "                                      DarkMatterAnnihilationSpectralModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mhYXAR3XiRC-",
   "metadata": {
    "id": "mhYXAR3XiRC-"
   },
   "outputs": [],
   "source": [
    "#%env GAMMAPY_DATA=/your_path/cta_school/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fc236",
   "metadata": {
    "id": "415fc236"
   },
   "source": [
    "# Exercise 1: Simulating a DM signal in the galactic centre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f91a01",
   "metadata": {
    "id": "d5f91a01"
   },
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9514cc",
   "metadata": {
    "id": "7a9514cc"
   },
   "outputs": [],
   "source": [
    "# Define basic simulation parameters\n",
    "livetime = 520.0 * u.hr\n",
    "GLON = 0.0 * u.deg\n",
    "GLAT = 0.0 * u.deg\n",
    "pointing = SkyCoord(0.0, 0.0, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "emin = 20/1000\n",
    "emax = 150\n",
    "\n",
    "energy_axes = MapAxis.from_energy_bounds(emin, emax, nbin=10, unit=\"TeV\", name=\"energy\")\n",
    "region = CircleSkyRegion(center=pointing, radius=10.0 * u.deg)\n",
    "\n",
    "geom = WcsGeom.create(\n",
    "    skydir=pointing,\n",
    "    binsz=0.02,\n",
    "    width=(20, 20),\n",
    "    frame=\"galactic\",\n",
    "    axes=[energy_axes],\n",
    ")\n",
    "\n",
    "empty = MapDataset.create(geom, name=\"dataset-simu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fa463",
   "metadata": {
    "id": "7a6fa463"
   },
   "source": [
    "The expected gamma-ray flux for the case in which the DM is annihilating is defined as:\n",
    "\n",
    "$\\frac{d\\Phi_\\gamma}{dE}=scale \\times J_{factor}\\;\\times \\dfrac{\\langle \\sigma v \\rangle}{4 \\pi \\; m_{\\rm DM}^2} \\sum_i {\\rm BR}_i \\dfrac{dN_\\gamma^{\\rm i}}{dE}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $scale=1$ (default), this will be the fit variable\n",
    "\n",
    "- $J_{factor}$ is the astrophysical factor\n",
    "\n",
    "- $m_{DM}$ is the DM particle mass\n",
    "\n",
    "- $i$ is the selected annihilation channel\n",
    "\n",
    "- $\\langle \\sigma v \\rangle= 3\\times 10^{-26}$ cm$^3$ s$^{-1}$ (default)\n",
    "\n",
    "- $BR=1$ (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f937d",
   "metadata": {
    "id": "e21f937d"
   },
   "outputs": [],
   "source": [
    "# **Define DM model**\n",
    "\n",
    "# Spectral\n",
    "JFAC = 5.5e+20 * u.Unit(\"GeV2 cm-5\") # Galactic centre [2007.16129] Einasto 10 deg\n",
    "mDM = 5000*u.Unit(\"GeV\")\n",
    "channel = \"b\"\n",
    "redshift = 0.0 # For extragalactic objects, we can add their redshift and also the EBL effect\n",
    "scale = 100 # As we want to make sure to have a signal\n",
    "dm_model = DarkMatterAnnihilationSpectralModel(\n",
    "    mass=mDM, \n",
    "    channel=channel, \n",
    "    jfactor=JFAC, \n",
    "    z=redshift,\n",
    "    scale=scale\n",
    ")\n",
    "\n",
    "# Plot spectral model to simulate\n",
    "fig_1 = plt.figure()\n",
    "plt.plot()\n",
    "dm_model.plot([emin*u.TeV, emax*u.TeV], energy_power=0)\n",
    "form = plt.FormatStrFormatter('$%g$')\n",
    "gca().xaxis.set_major_formatter(form)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1d2c3",
   "metadata": {
    "id": "e1e1d2c3"
   },
   "outputs": [],
   "source": [
    "# Spatial - We read the emission template created outside gammapy\n",
    "jfactor_filename = '/your_path/cta_school/annihil_gal2D_gNFW_gamma_1.26_Rodot_8.3kpc_rhoodot_0.4_LOS0_0_allsky_nside1024-JFACTOR_PER_SR-Jtot_per_sr-image-image.fits'\n",
    "hdul = fits.open(jfactor_filename)\n",
    "dm_spatial_model = TemplateSpatialModel.read(jfactor_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43879bf9",
   "metadata": {
    "id": "43879bf9"
   },
   "outputs": [],
   "source": [
    "# Set the sky model \n",
    "model_simu = SkyModel(spatial_model=dm_spatial_model, spectral_model=dm_model, name=\"gc\")\n",
    "print(model_simu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58b005",
   "metadata": {
    "id": "6f58b005"
   },
   "outputs": [],
   "source": [
    "# **Define BKG model**\n",
    "bkg_model = FoVBackgroundModel(dataset_name=\"dataset-simu\")\n",
    "print(bkg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ace7e",
   "metadata": {
    "id": "748ace7e"
   },
   "outputs": [],
   "source": [
    "# Gather all the models\n",
    "models = Models([model_simu, bkg_model])\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f1e84",
   "metadata": {
    "id": "e36f1e84"
   },
   "outputs": [],
   "source": [
    "# Load IRFs\n",
    "irfs = load_cta_irfs(\n",
    "    \"$GAMMAPY_DATA/Prod5-South-20deg-AverageAz-14MSTs37SSTs.180000s-v0.1.fits\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e170e",
   "metadata": {
    "id": "b80e170e"
   },
   "outputs": [],
   "source": [
    "# Create the observation\n",
    "obs = Observation.create(pointing=pointing, livetime=livetime, irfs=irfs)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad5318",
   "metadata": {
    "id": "c9ad5318"
   },
   "outputs": [],
   "source": [
    "# Make the MapDataset\n",
    "maker = MapDatasetMaker(selection=[\"exposure\", \"background\", \"psf\", \"edisp\"])\n",
    "dataset = maker.run(empty, obs)\n",
    "dataset.models = models\n",
    "dataset.fake(random_state=int(time.time())) # Here is where we combine models+IRFs and create a Poisson realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62835589",
   "metadata": {
    "id": "62835589"
   },
   "outputs": [],
   "source": [
    "# Save the created dataset\n",
    "dataset.write(\"/your_path/cta_school/dataset-gc_1.fits\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c46e2d",
   "metadata": {
    "id": "48c46e2d"
   },
   "source": [
    "## Inspect created dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91722ac",
   "metadata": {
    "id": "d91722ac"
   },
   "outputs": [],
   "source": [
    "# Let's make some plots!\n",
    "\n",
    "fig_peek, axs = plt.subplots(2,2, figsize=(7,7))\n",
    "\n",
    "img_1 = axs[0,0].imshow(\n",
    "    np.sum(dataset.counts.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr',\n",
    ")\n",
    "axs[0,0].set_title('Counts')\n",
    "divider = make_axes_locatable(axs[0,0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_1 = fig_peek.colorbar(img_1, cax=cax, orientation='vertical')\n",
    "\n",
    "img_2 = axs[0,1].imshow(\n",
    "    np.sum(dataset.background.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[0,1].set_title('Background')\n",
    "divider = make_axes_locatable(axs[0,1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_2 = fig_peek.colorbar(img_2, cax=cax, orientation='vertical')\n",
    "\n",
    "img_3 = axs[1,0].imshow(\n",
    "    np.sum(dataset.counts.data, axis=0) - np.sum(dataset.background.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[1,0].set_title('Excess')\n",
    "divider = make_axes_locatable(axs[1,0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_3 = fig_peek.colorbar(img_3, cax=cax, orientation='vertical')\n",
    "\n",
    "img_4 = axs[1,1].imshow(\n",
    "    np.sum(dataset.exposure.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[1,1].set_title('Exposure')\n",
    "divider = make_axes_locatable(axs[1,1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_4 = fig_peek.colorbar(img_4, cax=cax, orientation='vertical')\n",
    "cbar_4.ax.set_ylabel(r'm$^2$')\n",
    "\n",
    "fig_peek.subplots_adjust(wspace=0.45, hspace=0.2)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16792050",
   "metadata": {
    "id": "16792050"
   },
   "outputs": [],
   "source": [
    "# Let's check the spectrrum and the different contributions\n",
    "spec, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset.counts.get_spectrum().plot(label='Total counts')\n",
    "dataset.npred_background().get_spectrum().plot(label='BKG counts')\n",
    "#dataset.npred_signal().get_spectrum().plot(label='DM counts')\n",
    "axs.set_ylabel('Counts', fontsize=12)\n",
    "axs.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MBUXxPFcWaI8",
   "metadata": {
    "id": "MBUXxPFcWaI8"
   },
   "outputs": [],
   "source": [
    "dataset.excess.plot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885addd",
   "metadata": {
    "id": "c885addd"
   },
   "source": [
    "# Excercise 2: Analyze first provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1719f",
   "metadata": {
    "id": "06f1719f"
   },
   "outputs": [],
   "source": [
    "# Read the dataset prepared\n",
    "dataset_1 = MapDataset.read(\"/your_path/cta_school/dataset-gc_2.fits\")\n",
    "print(dataset_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534ac3e",
   "metadata": {
    "id": "c534ac3e"
   },
   "source": [
    "## Inspect a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97a452",
   "metadata": {
    "id": "3e97a452"
   },
   "outputs": [],
   "source": [
    "# Check properties of the dataset\n",
    "print(dataset_1.geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8110f63",
   "metadata": {
    "id": "e8110f63"
   },
   "outputs": [],
   "source": [
    "# Let's make some plots!\n",
    "\n",
    "fig_peek, axs = plt.subplots(2,2, figsize=(7,7))\n",
    "\n",
    "img_1 = axs[0,0].imshow(\n",
    "    np.sum(dataset_1.counts.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr',\n",
    ")\n",
    "axs[0,0].set_title('Counts')\n",
    "divider = make_axes_locatable(axs[0,0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_1 = fig_peek.colorbar(img_1, cax=cax, orientation='vertical')\n",
    "\n",
    "img_2 = axs[0,1].imshow(\n",
    "    np.sum(dataset_1.background.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[0,1].set_title('Background')\n",
    "divider = make_axes_locatable(axs[0,1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_2 = fig_peek.colorbar(img_2, cax=cax, orientation='vertical')\n",
    "\n",
    "img_3 = axs[1,0].imshow(\n",
    "    np.sum(dataset_1.counts.data, axis=0) - np.sum(dataset_1.background.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[1,0].set_title('Excess')\n",
    "divider = make_axes_locatable(axs[1,0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_3 = fig_peek.colorbar(img_3, cax=cax, orientation='vertical')\n",
    "\n",
    "img_4 = axs[1,1].imshow(\n",
    "    np.sum(dataset_1.exposure.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[1,1].set_title('Exposure')\n",
    "divider = make_axes_locatable(axs[1,1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_4 = fig_peek.colorbar(img_4, cax=cax, orientation='vertical')\n",
    "cbar_4.ax.set_ylabel(r'm$^2$')\n",
    "\n",
    "fig_peek.subplots_adjust(wspace=0.45, hspace=0.2)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba38b4",
   "metadata": {
    "id": "07ba38b4"
   },
   "source": [
    "Seems like we have something here, let's take a closer look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deeeb24",
   "metadata": {
    "id": "2deeeb24"
   },
   "outputs": [],
   "source": [
    "# Let's use the gammapy built-in functions to check the residuals \n",
    "spec_res_1, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset_1.plot_residuals_spectral(method=\"diff\", ax=axs)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d4c99",
   "metadata": {
    "id": "122d4c99"
   },
   "outputs": [],
   "source": [
    "spat_res_1, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset_1.plot_residuals_spatial(method=\"diff\", ax=axs)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf16b52",
   "metadata": {
    "id": "cbf16b52"
   },
   "outputs": [],
   "source": [
    "spec, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset_1.counts.get_spectrum().plot(label='Total counts')\n",
    "dataset_1.npred_background().get_spectrum().plot(label='BKG counts')\n",
    "axs.set_ylabel('Counts', fontsize=12)\n",
    "axs.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uwbwdHdsWUix",
   "metadata": {
    "id": "uwbwdHdsWUix"
   },
   "outputs": [],
   "source": [
    "dataset_1.excess.plot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149fcbd",
   "metadata": {
    "id": "e149fcbd"
   },
   "source": [
    "## Guess what's going on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b8d3a",
   "metadata": {
    "id": "f04b8d3a"
   },
   "source": [
    "Before searching for an specific DM signal (mass, channel and $\\langle \\sigma v \\rangle$), let's try a simple toy power-law point-like model because we first want an easy way to discard the background hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda15477",
   "metadata": {
    "id": "eda15477"
   },
   "outputs": [],
   "source": [
    "# **Define 3D Sky Model**\n",
    "\n",
    "# We define a power-law spectrum \n",
    "spectral_model_fit = PowerLawSpectralModel(index=2)\n",
    "\n",
    "# Define the point-like source\n",
    "spatial_model_fit = PointSpatialModel(lon_0=GLON, lat_0=GLAT, unit='deg', frame=\"galactic\")\n",
    "\n",
    "# Gather spectral and spatial models\n",
    "sky_model_fit = SkyModel(spatial_model=spatial_model_fit, spectral_model=spectral_model_fit, name='sky_model')\n",
    "\n",
    "# Add the background model extracted from the dataset\n",
    "bkg_model_fit = FoVBackgroundModel(dataset_name=dataset_1.name)\n",
    "\n",
    "# Combine source model + bkg model\n",
    "models_fit = Models([sky_model_fit, bkg_model_fit])\n",
    "\n",
    "# Let's fix the position of the source\n",
    "models_fit.parameters[\"lon_0\"].frozen = True\n",
    "models_fit.parameters[\"lat_0\"].frozen = True\n",
    "\n",
    "# Model with only bkg\n",
    "models_nosrc = Models([bkg_model_fit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23897a6c",
   "metadata": {
    "id": "23897a6c"
   },
   "outputs": [],
   "source": [
    "# Fit with the model with only background\n",
    "dataset_1.models = models_nosrc\n",
    "fit = Fit() # default is minuit minimizer\n",
    "result_fit_nosrc_1 = fit.run(datasets=[dataset_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce3aa",
   "metadata": {
    "id": "a45ce3aa"
   },
   "outputs": [],
   "source": [
    "# Fit with the model with background + point-like source\n",
    "dataset_1.models = models_fit\n",
    "fit = Fit()\n",
    "result_1 = fit.run(datasets=[dataset_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29499d",
   "metadata": {
    "id": "8e29499d"
   },
   "outputs": [],
   "source": [
    "# Let's perform the test likelihood ratio between both models\n",
    "\n",
    "# Gammapy returns the -2 log(L)\n",
    "logL_src = result_1.optimize_result.total_stat\n",
    "logL_nosrc = result_fit_nosrc_1.optimize_result.total_stat\n",
    "\n",
    "ts_1 = (logL_nosrc - logL_src)\n",
    "\n",
    "print(ts_1, np.sqrt(ts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cc04a",
   "metadata": {
    "id": "7b3cc04a"
   },
   "outputs": [],
   "source": [
    "# We can easely see the results of the Fit in table format\n",
    "result_1.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70915474",
   "metadata": {
    "id": "70915474"
   },
   "source": [
    "## Fit to a DM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a12be",
   "metadata": {
    "id": "9c1a12be"
   },
   "source": [
    "Clearly we have a signal! Now let's see if it is compatible with a DM signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267f8de",
   "metadata": {
    "id": "b267f8de"
   },
   "source": [
    "To make it more interesting, let's make a competition. Who obtain the highest TS for a DM model, wins!\n",
    "Remember, you can change the DM mass and channel of the model to fit the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de344b94",
   "metadata": {
    "id": "de344b94"
   },
   "outputs": [],
   "source": [
    "from re import X\n",
    "### \n",
    "mass_fit = X*u.GeV\n",
    "channel_fit = \"X\" \n",
    "        \n",
    "# Let's define the DM model to fit\n",
    "flux_model_fit = DarkMatterAnnihilationSpectralModel(mass=mass_fit, channel=channel_fit, jfactor=JFAC)\n",
    "        \n",
    "# Our source has spatial extension (read the input template for creating the simulation)\n",
    "dmmodel_fit = SkyModel(spatial_model = dm_spatial_model, \n",
    "                           spectral_model = flux_model_fit)\n",
    "        \n",
    "# Finally we add both models (background+source) \n",
    "models_fit_dm = Models([dmmodel_fit, bkg_model_fit])\n",
    "\n",
    "# We initialize the background parameters\n",
    "models_fit_dm.parameters['norm'].value = 1\n",
    "models_fit_dm.parameters['tilt'].value = 0\n",
    "\n",
    "# Add the models to our dataset!\n",
    "dataset_1.models = models_fit_dm\n",
    "\n",
    "# Let's fit\n",
    "fit = Fit()\n",
    "result_dm_1 = fit.run(datasets=[dataset_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81854096",
   "metadata": {
    "id": "81854096"
   },
   "outputs": [],
   "source": [
    "# We can easely see the results of the Fit in table format\n",
    "result_dm_1.parameters.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180893f9",
   "metadata": {
    "id": "180893f9"
   },
   "outputs": [],
   "source": [
    "# Let's perform the test likelihood ratio between both models\n",
    "\n",
    "# Gammapy returns the -2 log(L)\n",
    "logL_dm   = result_dm_1.optimize_result.total_stat\n",
    "\n",
    "ts_dm = (logL_nosrc - logL_dm)\n",
    "\n",
    "print(ts_dm, np.sqrt(ts_dm)) ### test statistic is too low, <5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43cd89",
   "metadata": {
    "id": "0f43cd89"
   },
   "source": [
    "# Excercise 2: Analyze second provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ff87b",
   "metadata": {
    "id": "ce2ff87b"
   },
   "outputs": [],
   "source": [
    "# Read the provided dataset\n",
    "dataset_2 = MapDataset.read(\"/your_path/cta_school/dataset-gc_3.fits\")\n",
    "print(dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7174b",
   "metadata": {
    "id": "0de7174b"
   },
   "source": [
    "## Inspect a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c57c7",
   "metadata": {
    "id": "df6c57c7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check some dataset properties\n",
    "print(dataset_2.geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c2253",
   "metadata": {
    "id": "ee1c2253"
   },
   "outputs": [],
   "source": [
    "# Let's make some plots!\n",
    "\n",
    "fig_peek, axs = plt.subplots(2,2, figsize=(6,6))\n",
    "\n",
    "img_1 = axs[0,0].imshow(\n",
    "    np.sum(dataset_2.counts.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr',\n",
    ")\n",
    "axs[0,0].set_title('Counts')\n",
    "divider = make_axes_locatable(axs[0,0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_1 = fig_peek.colorbar(img_1, cax=cax, orientation='vertical')\n",
    "\n",
    "img_2 = axs[0,1].imshow(\n",
    "    np.sum(dataset_2.background.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[0,1].set_title('Background')\n",
    "divider = make_axes_locatable(axs[0,1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_2 = fig_peek.colorbar(img_2, cax=cax, orientation='vertical')\n",
    "\n",
    "img_3 = axs[1,0].imshow(\n",
    "    np.sum(dataset_2.counts.data, axis=0) - np.sum(dataset_2.background.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[1,0].set_title('Excess')\n",
    "divider = make_axes_locatable(axs[1,0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_3 = fig_peek.colorbar(img_3, cax=cax, orientation='vertical')\n",
    "\n",
    "img_4 = axs[1,1].imshow(\n",
    "    np.sum(dataset_2.exposure.data, axis=0),\n",
    "    extent=(10.0+0.0,-10.0+0.0,10.0+0.0,-10.0+0.0),\n",
    "    origin = 'lower',\n",
    "    cmap='YlOrBr'\n",
    ")\n",
    "axs[1,1].set_title('Exposure')\n",
    "divider = make_axes_locatable(axs[1,1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cbar_4 = fig_peek.colorbar(img_4, cax=cax, orientation='vertical')\n",
    "cbar_4.ax.set_ylabel(r'm$^2$')\n",
    "\n",
    "fig_peek.subplots_adjust(wspace=0.45, hspace=0.2)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a5b54",
   "metadata": {
    "id": "310a5b54"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset_2.plot_residuals_spectral(method=\"diff\", ax=axs)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbc639",
   "metadata": {
    "id": "82cbc639"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset_2.plot_residuals_spatial(method=\"diff\", ax=axs)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ad897",
   "metadata": {
    "id": "e17ad897"
   },
   "outputs": [],
   "source": [
    "spec, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "dataset_2.counts.get_spectrum().plot(label='Total counts')\n",
    "dataset_2.npred_background().get_spectrum().plot(label='BKG counts')\n",
    "axs.set_ylabel('Counts', fontsize=12)\n",
    "axs.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67cae5",
   "metadata": {
    "id": "5d67cae5"
   },
   "outputs": [],
   "source": [
    "dataset_2.excess.plot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4368ac0",
   "metadata": {
    "id": "b4368ac0"
   },
   "source": [
    "## Guess what's going on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f86523",
   "metadata": {
    "id": "40f86523"
   },
   "source": [
    "Let's see if there is a signal on this data and we can guess what it is!\n",
    "Before searching for an specific DM signal (mass, channel and $\\langle \\sigma v \\rangle$), let's try the simple toy power-law point-like model we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66ece7",
   "metadata": {
    "id": "1b66ece7"
   },
   "outputs": [],
   "source": [
    "# Add the background model extracted from the dataset\n",
    "bkg_model_fit = FoVBackgroundModel(dataset_name=dataset_2.name)\n",
    "\n",
    "# Combine source model + bkg model\n",
    "models_fit = Models([sky_model_fit, bkg_model_fit])\n",
    "\n",
    "# Let's fix the position of the source\n",
    "models_fit.parameters[\"lon_0\"].frozen = True\n",
    "models_fit.parameters[\"lat_0\"].frozen = True\n",
    "\n",
    "# Model with only bkg\n",
    "models_nosrc = Models([bkg_model_fit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8c759",
   "metadata": {
    "id": "c5f8c759"
   },
   "outputs": [],
   "source": [
    "# Fit with the model with only background\n",
    "dataset_2.models = models_nosrc\n",
    "fit = Fit() # default is minuit minimizer\n",
    "result_fit_nosrc_2 = fit.run(datasets=[dataset_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74fb1f5",
   "metadata": {
    "id": "c74fb1f5"
   },
   "outputs": [],
   "source": [
    "# Fit with the model with background + point-like source\n",
    "dataset_2.models = models_fit\n",
    "fit = Fit()\n",
    "result_2 = fit.run(datasets=[dataset_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1a4c0",
   "metadata": {
    "id": "afb1a4c0"
   },
   "outputs": [],
   "source": [
    "# Let's perform the test likelihood ratio between both models\n",
    "\n",
    "# Gammapy returns the -2 log(L)\n",
    "logL_src   = result_2.optimize_result.total_stat\n",
    "logL_nosrc = result_fit_nosrc_2.optimize_result.total_stat\n",
    "\n",
    "ts_2 = (logL_nosrc - logL_src)\n",
    "\n",
    "print(ts_2, np.sqrt(ts_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44981b54",
   "metadata": {
    "id": "44981b54"
   },
   "source": [
    "The likelihood test values tell us that there is no preference for the detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6aa24d",
   "metadata": {
    "id": "fb6aa24d"
   },
   "outputs": [],
   "source": [
    "# We can easely see the results of the Fit in table format\n",
    "result_2.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e11bc",
   "metadata": {
    "id": "327e11bc"
   },
   "source": [
    "## Fit to a DM model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e29dd",
   "metadata": {
    "id": "4e3e29dd"
   },
   "source": [
    " \n",
    "Since $\\sqrt{(TS)}<5$ we have no positive detection, we shall produce an 95% CL upper limits to \"scale\" parameter by solving the equation:\n",
    "\n",
    "TS (scale) = 2.71\n",
    "    \n",
    "Then we can transform the limits on $scale$ to $\\rightarrow$ $\\langle \\sigma v \\rangle$ limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481b71d",
   "metadata": {
    "id": "5481b71d"
   },
   "outputs": [],
   "source": [
    "### \n",
    "mass_fit = 5000*u.GeV\n",
    "channel_fit = \"b\"\n",
    "        \n",
    "# Let's define the source model\n",
    "flux_model_fit = DarkMatterAnnihilationSpectralModel(mass=mass_fit, channel=channel_fit, jfactor=JFAC)\n",
    "        \n",
    "# Our source has spatial extension (read the input template for creating the simulation)\n",
    "dmmodel_fit = SkyModel(spatial_model = dm_spatial_model, \n",
    "                           spectral_model = flux_model_fit)\n",
    "        \n",
    "# Finally we add both models (background+source) \n",
    "models_fit_dm = Models([dmmodel_fit, bkg_model_fit])\n",
    "\n",
    "# WE initialize the background parameters\n",
    "models_fit_dm.parameters['norm'].value = 1\n",
    "models_fit_dm.parameters['tilt'].value = 0\n",
    "\n",
    "# Add the models to our dataset!\n",
    "dataset_2.models = models_fit_dm\n",
    "\n",
    "# Let's fit\n",
    "fit = Fit()\n",
    "result_dm_2 = fit.run(datasets=[dataset_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549011ce",
   "metadata": {
    "id": "549011ce"
   },
   "outputs": [],
   "source": [
    "# Finally let's perform the test likelihood ratio between both models.\n",
    "\n",
    "# Gammapy returns the -2 log(L)\n",
    "logL_dm   = result_dm_2.optimize_result.total_stat\n",
    "\n",
    "ts_dm_2 = (logL_nosrc - logL_dm)\n",
    "\n",
    "print(ts_dm_2, np.sqrt(ts_dm_2)) ### test statistic is too low, <5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019a0e5",
   "metadata": {
    "id": "4019a0e5"
   },
   "outputs": [],
   "source": [
    "# Again, We can check the result of the Fit:\n",
    "result_dm_2.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb831b5",
   "metadata": {
    "id": "5bb831b5"
   },
   "source": [
    "OMG why do we get a negative value for scale? How we can handle this?\n",
    "As we have not restricted the values where the fit can search in the whole space for the parameter... \n",
    "Let's learn two solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUR9w1sAYOG5",
   "metadata": {
    "id": "IUR9w1sAYOG5"
   },
   "outputs": [],
   "source": [
    "# Gammapy has a way to access the likelihood profile as an array\n",
    "# We need to define the number of entries and the minimum and maximum value:\n",
    "# The profile method does only allow positive values for the scale parameter\n",
    "\n",
    "dataset_2.models.parameters['scale'].scan_n_values = 100\n",
    "dataset_2.models.parameters['scale'].scan_min = 10e-6\n",
    "dataset_2.models.parameters['scale'].scan_max = 1000\n",
    "\n",
    "# We are ready to get the likelihood profile\n",
    "profile = fit.stat_profile(datasets=[dataset_2], parameter='scale', reoptimize=False)\n",
    "# this method runs on the parameters one at a time, while keeping other params fixed\n",
    "# if want to change then reoptimize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04013fee",
   "metadata": {
    "id": "04013fee"
   },
   "outputs": [],
   "source": [
    "total_stat  = dataset_2.stat_sum() # This is the likelihood of the fit -> (-2ln(L))\n",
    "xvals = profile[\"P2gyc93K.spectral.scale_scan\"]\n",
    "y_profile = profile[\"stat_scan\"]\n",
    "yvals = profile[\"stat_scan\"] - total_stat #stat_scan is the value of the likelihood in the maximum (minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41703542",
   "metadata": {
    "id": "41703542"
   },
   "outputs": [],
   "source": [
    "# To reach to see our minimum, we add it manually to the array\n",
    "xvals = np.insert(xvals, 0, result_dm_2.parameters['scale'].value)\n",
    "yvals = np.insert(yvals, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d35b3",
   "metadata": {
    "id": "012d35b3"
   },
   "outputs": [],
   "source": [
    "# Some useful values\n",
    "min_xval = np.min(xvals)\n",
    "max_xval = np.max(xvals)\n",
    "min_yval = np.min(yvals)\n",
    "max_yval = np.max(yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1f0cb",
   "metadata": {
    "id": "39b1f0cb"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(xvals,yvals)\n",
    "plt.ylabel(\"Likelihood profile\", fontsize=12)\n",
    "plt.xlabel(\"Scale parameter\", fontsize=12)\n",
    "#plt.xlim(-50, 600)\n",
    "\n",
    "plt.vlines(result_dm_2.parameters['scale'].value, 0, max_yval, ls=\"--\", color='red')\n",
    "plt.vlines(0, 0, max_yval, ls=\"--\", color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a94c1",
   "metadata": {
    "id": "e50a94c1"
   },
   "source": [
    "### Unbounded likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da33156",
   "metadata": {
    "id": "7da33156"
   },
   "source": [
    "Let's use the real minimum value that we have, and search for the solution\n",
    "We recall this will only be valid if the obtain limits is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6435",
   "metadata": {
    "id": "b8ab6435"
   },
   "outputs": [],
   "source": [
    "# Let's found the scale value corresponding to the 95% C.L \n",
    "# We don't want the corresponding negative value of the solution\n",
    "# This corresponds to solve the next equation (one-sided distribution)\n",
    "# Recall we only want to interpolate, son min and max values should be within the values that xvals conatins already\n",
    "\n",
    "scale_found = brentq(interp1d(xvals, yvals-2.71, kind=\"quadratic\"),\n",
    "                    min_xval, max_xval,\n",
    "                    maxiter=100,\n",
    "                    rtol=1e-5,)\n",
    "\n",
    "print('Check the scale parameter: ', scale_found)\n",
    "sigma_v_ul = scale_found * DarkMatterAnnihilationSpectralModel.THERMAL_RELIC_CROSS_SECTION\n",
    "sigma_v_ul = sigma_v_ul.value\n",
    "print('!!!!!!!!!!!!!! Final Results!!!!!!!!!!')\n",
    "print(channel_fit, mass_fit, scale_found, sigma_v_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba7595",
   "metadata": {
    "id": "19ba7595"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(xvals, yvals)\n",
    "plt.ylabel(\"Likelihood profile\", fontsize=12)\n",
    "plt.xlabel(\"Scale parameter\", fontsize=12)\n",
    "\n",
    "# Line corresponding to 95% CL from minimum\n",
    "plt.hlines(2.71, result_dm_2.parameters['scale'].value, max_xval, ls=\"--\", color='red')\n",
    "plt.hlines(0, result_dm_2.parameters['scale'].value, max_xval, ls=\"--\", color='red')\n",
    "plt.vlines(result_dm_2.parameters['scale'].value, 0, max_yval, ls=\"--\", color='red')\n",
    "plt.vlines(scale_found, 0, max_yval, ls=\"--\", color='red')\n",
    "\n",
    "#plt.ylim(-10, 100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c25cc",
   "metadata": {
    "id": "561c25cc"
   },
   "source": [
    "### Bounded likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6f6be",
   "metadata": {
    "id": "0ba6f6be"
   },
   "source": [
    "We set the as best value scale = 0, and search the UL from this\n",
    "We this will be always valid, and produce more conservative limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0608c6",
   "metadata": {
    "id": "bf0608c6"
   },
   "outputs": [],
   "source": [
    "y_profile = profile[\"stat_scan\"]\n",
    "y_profile = np.insert(y_profile, 0, total_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdff8c",
   "metadata": {
    "id": "c5cdff8c"
   },
   "outputs": [],
   "source": [
    "profile_interp = interp1d(xvals, y_profile, kind=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1040b17",
   "metadata": {
    "id": "e1040b17"
   },
   "outputs": [],
   "source": [
    "# We need to reset the yvals to the value at scale = 0 instead of best_stat\n",
    "yvals_rescaled = y_profile -  profile_interp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eea5db",
   "metadata": {
    "id": "f2eea5db"
   },
   "outputs": [],
   "source": [
    "# Let's found the scale value corresponding to the 95% C.L \n",
    "\n",
    "scale_found_2 = brentq(interp1d(xvals, yvals_rescaled-2.71, kind=\"quadratic\"),\n",
    "                    min_xval, max_xval,\n",
    "                    maxiter=100,\n",
    "                    rtol=1e-5,)\n",
    "\n",
    "print('Check the scale parameter: ', scale_found)\n",
    "sigma_v_ul_2 = scale_found_2 * DarkMatterAnnihilationSpectralModel.THERMAL_RELIC_CROSS_SECTION\n",
    "sigma_v_ul_2 = sigma_v_ul_2.value\n",
    "print('!!!!!!!!!!!!!! Final Results!!!!!!!!!!')\n",
    "print(channel_fit, mass_fit, scale_found_2, sigma_v_ul_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cf396",
   "metadata": {
    "id": "820cf396"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(xvals, yvals_rescaled)\n",
    "plt.ylabel(\"Likelihood profile\", fontsize=12)\n",
    "plt.xlabel(\"Scale parameter\", fontsize=12)\n",
    "\n",
    "# Line corresponding to 95% CL from minimum\n",
    "plt.hlines(2.71, result_dm_2.parameters['scale'].value, max_xval, ls=\"--\", color='red')\n",
    "plt.hlines(0, result_dm_2.parameters['scale'].value, max_xval, ls=\"--\", color='red')\n",
    "plt.vlines(0, 0, np.max(yvals_rescaled), ls=\"--\", color='red')\n",
    "plt.vlines(scale_found_2, 0, np.max(yvals_rescaled), ls=\"--\", color='red')\n",
    "\n",
    "#plt.ylim(-10, 100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686174ad",
   "metadata": {
    "id": "686174ad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
